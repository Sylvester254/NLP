{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved News-Articles/sports.txt\n",
      "Successfully saved News-Articles/sports.txt\n",
      "Successfully saved News-Articles/sports.txt\n",
      "Successfully saved News-Articles/business.txt\n",
      "Successfully saved News-Articles/business.txt\n",
      "Successfully saved News-Articles/business.txt\n",
      "Successfully saved News-Articles/technology.txt\n",
      "Successfully saved News-Articles/technology.txt\n",
      "Successfully saved News-Articles/technology.txt\n",
      "Successfully saved News-Articles/technology.txt\n",
      "Successfully saved News-Articles/sports.txt\n",
      "Successfully saved News-Articles/sports.txt\n",
      "Successfully saved News-Articles/politics.txt\n",
      "Successfully saved News-Articles/politics.txt\n",
      "Successfully saved News-Articles/politics.txt\n",
      "Successfully saved News-Articles/politics.txt\n",
      "Successfully saved News-Articles/health.txt\n",
      "Successfully saved News-Articles/health.txt\n",
      "Successfully saved News-Articles/health.txt\n",
      "Successfully saved News-Articles/health.txt\n",
      "Successfully saved News-Articles/climate.txt\n",
      "Successfully saved News-Articles/climate.txt\n",
      "Successfully saved News-Articles/climate.txt\n",
      "Successfully saved News-Articles/sports.txt\n",
      "Successfully saved News-Articles/climate.txt\n",
      "Successfully saved News-Articles/climate.txt\n",
      "Successfully saved News-Articles/climate.txt\n",
      "Successfully saved News-Articles/climate.txt\n",
      "Successfully saved News-Articles/business.txt\n",
      "Successfully saved News-Articles/business.txt\n",
      "Successfully saved News-Articles/business.txt\n",
      "Successfully saved News-Articles/sports.txt\n",
      "Successfully saved News-Articles/politics.txt\n",
      "Successfully saved News-Articles/technology.txt\n",
      "Successfully saved News-Articles/technology.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "# create the directory if it doesn't already exist\n",
    "if not os.path.exists('News-Articles'):\n",
    "    os.makedirs('News-Articles')\n",
    "\n",
    "\n",
    "# Load URLs from JSON file\n",
    "with open('urls.json', 'r') as f:\n",
    "    urls = json.load(f)\n",
    "\n",
    "for url in urls:\n",
    "    url_string = url[\"url\"]\n",
    "    r = requests.get(url_string)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        article = soup.find(\"article\")\n",
    "        # Get the text out of the soup and print it\n",
    "        content = article.get_text()\n",
    "        # print(content)\n",
    "        \n",
    "        # construct the path to the file within the News-Articles directory\n",
    "        filename = os.path.join('News-Articles', url['title'] + '.txt')\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write(content)\n",
    "            f.write('\\n\\n')\n",
    "        print(f'Successfully saved {filename}')\n",
    "    else:\n",
    "        print(f\"Error: {r.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created news_corpus.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Directory where the .txt files are located\n",
    "directory = './News-Articles/'\n",
    "\n",
    "# Create a list to store the rows of the CSV file\n",
    "csv_rows = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        # Read the contents of the file\n",
    "        with open(os.path.join(directory, filename), 'r') as f:\n",
    "            content = f.read().replace('\\n', ' ')\n",
    "        \n",
    "        # Get the title from the filename\n",
    "        title = filename.split('.')[0]\n",
    "        \n",
    "        # Append the title and content to the CSV row list\n",
    "        csv_rows.append([title, content])\n",
    "\n",
    "# Write the CSV file\n",
    "with open('news_corpus.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Topic', 'Content'])  # write the header row\n",
    "    writer.writerows(csv_rows)  # write the content rows\n",
    "    print(\"Successfully created news_corpus.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 104]\n",
      "[nltk_data]     Connection reset by peer>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Topic                                            Content\n",
      "0    business  happening house price could crashpublished hou...\n",
      "1     climate  climate change england ready impactpublished d...\n",
      "2      health  blood test may spare cancer patient chemopubli...\n",
      "3    politics  donald trump indictment exus president charged...\n",
      "4  technology  elon musk among expert urging halt ai training...\n",
      "5      sports  manchester united premier league club owe almo...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load the news corpus into a Pandas DataFrame\n",
    "df = pd.read_csv('news_corpus.csv')\n",
    "\n",
    "# Initialize NLTK components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_text(Content):\n",
    "    # Remove punctuation\n",
    "    Content = Content.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove digits\n",
    "    Content = re.sub(r'\\d+', '', Content)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(Content.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    # Lemmatize the remaining words\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "    # Rejoin the words into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the preprocessing function to the news corpus and save into a new CSV file\n",
    "df['Content'] = df['Content'].apply(preprocess_text)\n",
    "df.to_csv('preprocessed_news_corpus.csv', index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m y \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[39m# split data into training and test sets\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m, stratify\u001b[39m=\u001b[39;49my)\n\u001b[1;32m     30\u001b[0m \u001b[39m# Train the Naive Bayes classifier\u001b[39;00m\n\u001b[1;32m     31\u001b[0m clf \u001b[39m=\u001b[39m MultinomialNB()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2583\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2579\u001b[0m         CVClass \u001b[39m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2581\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2583\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X\u001b[39m=\u001b[39;49marrays[\u001b[39m0\u001b[39;49m], y\u001b[39m=\u001b[39;49mstratify))\n\u001b[1;32m   2585\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   2586\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2587\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:1689\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[39m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \n\u001b[1;32m   1661\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1686\u001b[0m \u001b[39mto an integer.\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1689\u001b[0m \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1690\u001b[0m     \u001b[39myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2078\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2076\u001b[0m class_counts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbincount(y_indices)\n\u001b[1;32m   2077\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmin(class_counts) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> 2078\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2079\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe least populated class in y has only 1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2080\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m member, which is too few. The minimum\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2081\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m number of groups for any class cannot\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2082\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m be less than 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2083\u001b[0m     )\n\u001b[1;32m   2085\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m<\u001b[39m n_classes:\n\u001b[1;32m   2086\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2087\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe train_size = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m should be greater or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mequal to the number of classes = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n_train, n_classes)\n\u001b[1;32m   2089\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "# Load the preprocessed news corpus\n",
    "df = pd.read_csv('preprocessed_news_corpus.csv')\n",
    "\n",
    "# Create a TF-IDF Vectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(2, 3))\n",
    "# vectorizer = TfidfVectorizer(max_df=0.5)\n",
    "# vectorizer = TfidfVectorizer(min_df=3)\n",
    "\n",
    "\n",
    "# Vectorize the text data\n",
    "X = vectorizer.fit_transform(df['Content'])\n",
    "\n",
    "# Get the labels\n",
    "y = df['Topic']\n",
    "\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the categories for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting an accuracy, precision, recall, and F1-score of 0.0 meanig that the classifier did not correctly predict any of the classes in the test set.\n",
    "I havent figured why since adjusting the parameters of the TF-IDF Vectorizer to improve the quality of the features did not make any difference.\n",
    "Let me try using Bag-of-Words (BoW) model with CountVectorizer instead and see how it turns out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silver/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/silver/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the preprocessed news corpus\n",
    "df = pd.read_csv('preprocessed_news_corpus.csv')\n",
    "\n",
    "# Create a Count Vectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Vectorize the text data\n",
    "X = vectorizer.fit_transform(df['Content'])\n",
    "\n",
    "# Get the labels\n",
    "y = df['Topic']\n",
    "\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the categories for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business      1\n",
       "climate       1\n",
       "health        1\n",
       "politics      1\n",
       "technology    1\n",
       "sports        1\n",
       "Name: Topic, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed news corpus\n",
    "df = pd.read_csv('preprocessed_news_corpus.csv')\n",
    "\n",
    "# check for class imbalance\n",
    "df['Topic'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the issue all along. my corpus is imbalanced. fixable by increasing the size of my dataset by adding more instances of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silver/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/silver/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the preprocessed news corpus\n",
    "df = pd.read_csv('preprocessed_news_corpus.csv')\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Vectorize the text data\n",
    "X = vectorizer.fit_transform(df['Content'])\n",
    "\n",
    "# Get the labels\n",
    "y = df['Topic']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the categories for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred, average='weighted'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
