Elon Musk among experts urging a halt to AI trainingPublished8 hours agocommentsCommentsShareclose panelShare pageCopy linkAbout sharingImage source, AFPImage caption, Elon Musk is among those warning of the risks from advanced AIBy Chris VallanceTechnology reporterKey figures in artificial intelligence want training of powerful AI systems to be suspended amid fears of a threat to humanity.They have signed an open letter warning of potential risks, and say the race to develop AI systems is out of control.Twitter chief Elon Musk is among those who want training of AIs above a certain capacity to be halted for at least six months.Apple co-founder Steve Wozniak and some researchers at DeepMind also signed.OpenAI, the company behind ChatGPT, recently released GPT-4 - a state-of-the-art technology, which has impressed observers with its ability to do tasks such as answering questions about objects in images.The letter, from Future of Life Institute and signed by the luminaries, wants development to be halted temporarily at that level, warning in their letter of the risks future, more advanced systems might pose."AI systems with human-competitive intelligence can pose profound risks to society and humanity," it says.The Future of Life Institute is a not-for-profit organisation which says its mission is to "steer transformative technologies away from extreme, large-scale risks and towards benefiting life". This video can not be playedTo play this video you need to enable JavaScript in your browser.Media caption, Watch: What is artificial intelligence?Mr Musk, owner of Twitter and chief executive of car company Tesla, is listed as an external adviser to the organisation.Advanced AIs need to be developed with care, the letter says, but instead, "recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no-one - not even their creators - can understand, predict, or reliably control".The letter warns that AIs could flood information channels with misinformation, and replace jobs with automation.Is the world prepared for the coming AI storm?The letter follows a recent report from investment bank Goldman Sachs which said that while AI was likely to increase productivity, millions of jobs could become automated.However, other experts told the BBC the effect of AI on the labour market was very hard to predict. Outsmarted and obsoleteMore speculatively, the letter asks: "Should we develop non-human minds that might eventually outnumber, outsmart, obsolete [sic] and replace us?"Stuart Russell, computer-science professor at the University of California, Berkeley, and a signatory to the letter, told BBC News: "AI systems pose significant risks to democracy through weaponised disinformation, to employment through displacement of human skills and to education through plagiarism and demotivation."And in the future, advanced AI's may pose a "more general threat to human control over our civilization". "In the long run, taking sensible precautions is a small price to pay to mitigate these risks," Prof Russell added.But Princeton computer-science professor Arvind Narayanan accused the letter of focusing on "speculative, futuristic risk, ignoring the version of the problem that is already harming people".'Slow down'In a recent blog post quoted in the letter, OpenAI warned of the risks if an artificial general intelligence (AGI) were developed recklessly: "A misaligned superintelligent AGI could cause grievous harm to the world; an autocratic regime with a decisive superintelligence lead could do that, too."Co-ordination among AGI efforts to slow down at critical junctures will likely be important," the firm wrote.OpenAI has not publicly commented on the letter. The BBC has asked the firm whether it backs the call.Mr Musk was a co-founder of OpenAI - though he resigned from the board of the organisation some years ago and has tweeted critically about its current direction. Autonomous driving functions made by his car company Tesla, like most similar systems, use AI technology.The letter asks AI labs "to immediately pause for at least six months the training of AI systems more powerful than GPT-4".If such a delay cannot be enacted quickly, governments should step in and institute a moratorium, it says."New and capable regulatory authorities dedicated to AI" would also be needed. Recently, a number of proposals for the regulation of technology have been put forward in the US, UK and EU. However, the UK has ruled out a dedicated regulator for AI.Related TopicsElon MuskArtificial intelligenceMore on this storyUK rules out new AI regulator1 day agoAI could affect 300 million jobs - report2 days agoBill Gates: AI most important tech advance in decades21 MarchIs the world prepared for the coming AI storm?16 MarchView comments

Elon Musk: Twitter boss announces blue tick shake-upPublished2 days agocommentsCommentsShareclose panelShare pageCopy linkAbout sharingImage source, Getty ImagesAnnabelle LiangBusiness reporterTwitter boss Elon Musk has announced a shake-up of the social media platform's paid Twitter Blue feature.From 15 April only verified subscribers will have posts recommended to other users and be allowed to vote in polls.Under the policy, posts from non-paying accounts will not be included in the "For you" stream of recommended tweets.Last week, the firm said it would remove the verified status of some "legacy" accounts, which date from before Mr Musk bought the firm.Users currently pay $7 (£5.70) a month for blue-tick verification, which also allows access to additional features.Mr Musk said the changes were "the only realistic way to address advanced AI bot swarms taking over. It is otherwise a hopeless losing battle.""Voting in polls will require verification for same reason," he added.In an earlier post, Mr Musk said paid verification significantly increases the cost of using bots and makes it easier to identify them.Skip twitter post by Elon MuskAllow Twitter content?This article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitter’s cookie policy, external and privacy policy, external before accepting. To view this content choose ‘accept and continue’.Accept and continueThe BBC is not responsible for the content of external sites.End of twitter post by Elon MuskHowever, the move has been criticised by some social media users.A former worker on Twitter's verification team who asked to remain anonymous told the BBC: "Our number one goal for my team was to protect users from real world harm and this screams the complete opposite to me.""Verified users will use their power and their presence on the platform to influence anything from misinformation to actual harm for users all around the world. It's a silent threat that no one is seeing," they added.When Elon Musk took over Twitter, he was full of lofty ambitions about bringing back "free speech" .He wanted the platform to be "maximally trusted" and said this "isn't a way to make money".Yet Twitter policy now feels very different to those ideals.Realising that it was difficult to increase advertising revenue, Musk turned to a subscription based model.The sell initially was to give users a "blue tick" verification, if they paid a monthly fee.But the paying users didn't come in their droves. Stuck with flagging advertising revenue, and a stuttering subscription model - Musk has decided to take the nuclear option.There are two algorithms on Twitter - the "For you" stream of recommended tweets and stream of tweets from people you follow.Elon Musk's new policy will now essentially preclude non-paying users from taking part in one of those streams.It means unverified Twitter users will be far less likely to have their tweets liked or retweeted.Combined with Elon Musk's winding down of misinformation checks - this could be an extremely dangerous moment for Twitter.I've spoken to former employees who think this could be manna from heaven for trolls and people pushing misinformation.It also raises an existential question for Twitter. It was supposed to be fairly meritocratic place - with tweets rising by the quality of their content.It was a big part of Twitter's success. But now, that appears to have been swept away.Twitter Blue had a chaotic initial launch in November, as people started impersonating big brands and celebrities and paying for the blue tick badge in order to make them look authentic. Many pretended to be Mr Musk himself.This forced Twitter to pause the feature after less than a week, before it was relaunched the following month.Twitter Blue has since been used by controversial groups, including Taliban officials and their prominent supporters in Afghanistan.Verified users have their tweets amplified above other accounts. Subscribers also have access to additional features including an edit button, among other perks.Previously, the blue tick was used to indicate that high-profile accounts were authentic. It was given out by Twitter without a subscription fee - but only the firm itself decided who got one.Last week, Twitter said it would also start phasing out its "legacy verified programme" and remove some "legacy verified checkmarks" from 1 April.The firm added that users needed to pay to "keep your blue checkmark on Twitter".Bots on social media platforms can hurt their ability to grow advertising revenue or paid-for subscriptions.Mr Musk has often expressed concerns about how many fake or spam accounts are on Twitter. At one point he put his $44bn plan to buy the social media platform on hold as he queried the number of bots disclosed by the firm's previous management team.Related TopicsSocial mediaElon MuskTwitterMore on this storyTwitter says parts of source code leaked online3 days agoMeta exploring plans for Twitter rival10 MarchMusk apologises to Twitter worker over online row8 MarchTaliban start buying blue ticks on Twitter16 JanuaryView comments

Why fun apps are banned on French officials' phonesPublished3 days agoShareclose panelShare pageCopy linkAbout sharingImage source, Getty ImagesBy Shiona McCallumTechnology reporterGovernments and institutions around the world have been banning TikTok from officials' phones and devices, over the past few months.The Netherlands and Norway have joined the list of countries banning the video-sharing app, owned by Chinese company ByteDance, from government-issued devices.And despite TikTok's insistence it is independently run and shares no users' data with the Chinese government, France has become the latest country to restrict officials' access to it. But at the same time, it has also become the first country to do the same for all "recreational" apps - including:NetflixInstagramCandy CrushTwitterThe ban, monitored by France's cyber-security agency, will affect about 2.5 million civil servants."Recreational applications do not deliver sufficient levels of cyber-security and data protection to be deployed on administration's digital tools," Civil Service Minister Stanislas Guerini said."These applications may therefore constitute a risk to the data protection of these administrations and their public officials." Although, exceptions could be granted for "institutional communications" purposes.So it looks like France, unlike the rest of the West, is viewing Chinese and American technology companies in a similar way. Although, it is not the first instance of technology tension between the US and France.In 2019, France approved a digital-services tax despite threats of retaliation by the US, who argued it unfairly targeted American technology giants such as Google and Facebook.And since 2018, France has been vocally opposed to the US Cloud Act, which gives American law-enforcement authorities the power to request data stored by most major cloud providers, even if it is outside the US.France has had a prominent voice in the Gaia-X project - set up to:address the conflict between the US Cloud Act and the EU General Data Protection Regulation (GDPR)set European cloud standards ensure customers' data is stored and processed in Europe, "immune" from non-European lawsDay after day, governments and public bodies are banning TikTok from staff devices - but France's decision to clamp down on all "recreational apps" stands out and is being praised by privacy campaigners.Fears about TikTok have reached fever pitch - but alongside the furore, people are beginning to question the privacy practices of other apps too - regardless of where the companies are based. And this feels like a moment privacy campaigners have been waiting for.Questions are being asked about how data is collected and used by the likes of Facebook, Instagram, Snapchat and even Candy Crush.The political focus is currently on TikTok because its parent company is in China - but the French government is clearly saying all these social-media companies have questions to answer.Related TopicsFrench politicsTikTokFranceChinaAppsMore on this storyWhat could a TikTok ban mean for creators?7 days agoFive takeaways from TikTok CEO's Congress grilling6 days agoHow a TikTok ban would - or wouldn't - work in practice7 days ago

Elon Musk among experts urging a halt to AI trainingPublished21 hours agocommentsCommentsShareclose panelShare pageCopy linkAbout sharingImage source, AFPImage caption, Elon Musk is among those warning of the risks from advanced AIBy Chris VallanceTechnology reporterKey figures in artificial intelligence want training of powerful AI systems to be suspended amid fears of a threat to humanity.They have signed an open letter warning of potential risks, and say the race to develop AI systems is out of control.Twitter chief Elon Musk is among those who want training of AIs above a certain capacity to be halted for at least six months.Apple co-founder Steve Wozniak and some researchers at DeepMind also signed.OpenAI, the company behind ChatGPT, recently released GPT-4 - a state-of-the-art technology, which has impressed observers with its ability to do tasks such as answering questions about objects in images.The letter, from Future of Life Institute and signed by the luminaries, wants development to be halted temporarily at that level, warning in their letter of the risks future, more advanced systems might pose."AI systems with human-competitive intelligence can pose profound risks to society and humanity," it says.The Future of Life Institute is a not-for-profit organisation which says its mission is to "steer transformative technologies away from extreme, large-scale risks and towards benefiting life". This video can not be playedTo play this video you need to enable JavaScript in your browser.Media caption, Watch: What is artificial intelligence?Mr Musk, owner of Twitter and chief executive of car company Tesla, is listed as an external adviser to the organisation.Advanced AIs need to be developed with care, the letter says, but instead, "recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no-one - not even their creators - can understand, predict, or reliably control".The letter warns that AIs could flood information channels with misinformation, and replace jobs with automation.Is the world prepared for the coming AI storm?The letter follows a recent report from investment bank Goldman Sachs which said that while AI was likely to increase productivity, millions of jobs could become automated.However, other experts told the BBC the effect of AI on the labour market was very hard to predict. Outsmarted and obsoleteMore speculatively, the letter asks: "Should we develop non-human minds that might eventually outnumber, outsmart, obsolete [sic] and replace us?"Stuart Russell, computer-science professor at the University of California, Berkeley, and a signatory to the letter, told BBC News: "AI systems pose significant risks to democracy through weaponised disinformation, to employment through displacement of human skills and to education through plagiarism and demotivation."And in the future, advanced AI's may pose a "more general threat to human control over our civilization". "In the long run, taking sensible precautions is a small price to pay to mitigate these risks," Prof Russell added.But Princeton computer-science professor Arvind Narayanan accused the letter of focusing on "speculative, futuristic risk, ignoring the version of the problem that is already harming people".'Slow down'In a recent blog post quoted in the letter, OpenAI warned of the risks if an artificial general intelligence (AGI) were developed recklessly: "A misaligned superintelligent AGI could cause grievous harm to the world; an autocratic regime with a decisive superintelligence lead could do that, too."Co-ordination among AGI efforts to slow down at critical junctures will likely be important," the firm wrote.OpenAI has not publicly commented on the letter. The BBC has asked the firm whether it backs the call.Mr Musk was a co-founder of OpenAI - though he resigned from the board of the organisation some years ago and has tweeted critically about its current direction. Autonomous driving functions made by his car company Tesla, like most similar systems, use AI technology.The letter asks AI labs "to immediately pause for at least six months the training of AI systems more powerful than GPT-4".If such a delay cannot be enacted quickly, governments should step in and institute a moratorium, it says."New and capable regulatory authorities dedicated to AI" would also be needed. Recently, a number of proposals for the regulation of technology have been put forward in the US, UK and EU. However, the UK has ruled out a dedicated regulator for AI.Related TopicsElon MuskArtificial intelligenceMore on this storyUK rules out new AI regulator2 days agoAI could affect 300 million jobs - report2 days agoBill Gates: AI most important tech advance in decades21 MarchIs the world prepared for the coming AI storm?16 MarchView comments

Elon Musk: Twitter boss announces blue tick shake-upPublished3 days agocommentsCommentsShareclose panelShare pageCopy linkAbout sharingImage source, Getty ImagesAnnabelle LiangBusiness reporterTwitter boss Elon Musk has announced a shake-up of the social media platform's paid Twitter Blue feature.From 15 April only verified subscribers will have posts recommended to other users and be allowed to vote in polls.Under the policy, posts from non-paying accounts will not be included in the "For you" stream of recommended tweets.Last week, the firm said it would remove the verified status of some "legacy" accounts, which date from before Mr Musk bought the firm.Users currently pay $7 (£5.70) a month for blue-tick verification, which also allows access to additional features.Mr Musk said the changes were "the only realistic way to address advanced AI bot swarms taking over. It is otherwise a hopeless losing battle.""Voting in polls will require verification for same reason," he added.In an earlier post, Mr Musk said paid verification significantly increases the cost of using bots and makes it easier to identify them.Skip twitter post by Elon MuskAllow Twitter content?This article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitter’s cookie policy, external and privacy policy, external before accepting. To view this content choose ‘accept and continue’.Accept and continueThe BBC is not responsible for the content of external sites.End of twitter post by Elon MuskHowever, the move has been criticised by some social media users.A former worker on Twitter's verification team who asked to remain anonymous told the BBC: "Our number one goal for my team was to protect users from real world harm and this screams the complete opposite to me.""Verified users will use their power and their presence on the platform to influence anything from misinformation to actual harm for users all around the world. It's a silent threat that no one is seeing," they added.When Elon Musk took over Twitter, he was full of lofty ambitions about bringing back "free speech" .He wanted the platform to be "maximally trusted" and said this "isn't a way to make money".Yet Twitter policy now feels very different to those ideals.Realising that it was difficult to increase advertising revenue, Musk turned to a subscription based model.The sell initially was to give users a "blue tick" verification, if they paid a monthly fee.But the paying users didn't come in their droves. Stuck with flagging advertising revenue, and a stuttering subscription model - Musk has decided to take the nuclear option.There are two algorithms on Twitter - the "For you" stream of recommended tweets and stream of tweets from people you follow.Elon Musk's new policy will now essentially preclude non-paying users from taking part in one of those streams.It means unverified Twitter users will be far less likely to have their tweets liked or retweeted.Combined with Elon Musk's winding down of misinformation checks - this could be an extremely dangerous moment for Twitter.I've spoken to former employees who think this could be manna from heaven for trolls and people pushing misinformation.It also raises an existential question for Twitter. It was supposed to be fairly meritocratic place - with tweets rising by the quality of their content.It was a big part of Twitter's success. But now, that appears to have been swept away.Twitter Blue had a chaotic initial launch in November, as people started impersonating big brands and celebrities and paying for the blue tick badge in order to make them look authentic. Many pretended to be Mr Musk himself.This forced Twitter to pause the feature after less than a week, before it was relaunched the following month.Twitter Blue has since been used by controversial groups, including Taliban officials and their prominent supporters in Afghanistan.Verified users have their tweets amplified above other accounts. Subscribers also have access to additional features including an edit button, among other perks.Previously, the blue tick was used to indicate that high-profile accounts were authentic. It was given out by Twitter without a subscription fee - but only the firm itself decided who got one.Last week, Twitter said it would also start phasing out its "legacy verified programme" and remove some "legacy verified checkmarks" from 1 April.The firm added that users needed to pay to "keep your blue checkmark on Twitter".Bots on social media platforms can hurt their ability to grow advertising revenue or paid-for subscriptions.Mr Musk has often expressed concerns about how many fake or spam accounts are on Twitter. At one point he put his $44bn plan to buy the social media platform on hold as he queried the number of bots disclosed by the firm's previous management team.Related TopicsSocial mediaElon MuskTwitterMore on this storyTwitter says parts of source code leaked online4 days agoMeta exploring plans for Twitter rival10 MarchMusk apologises to Twitter worker over online row8 MarchTaliban start buying blue ticks on Twitter16 JanuaryView comments

Why fun apps are banned on French officials' phonesPublished3 days agoShareclose panelShare pageCopy linkAbout sharingImage source, Getty ImagesBy Shiona McCallumTechnology reporterGovernments and institutions around the world have been banning TikTok from officials' phones and devices, over the past few months.The Netherlands and Norway have joined the list of countries banning the video-sharing app, owned by Chinese company ByteDance, from government-issued devices.And despite TikTok's insistence it is independently run and shares no users' data with the Chinese government, France has become the latest country to restrict officials' access to it. But at the same time, it has also become the first country to do the same for all "recreational" apps - including:NetflixInstagramCandy CrushTwitterThe ban, monitored by France's cyber-security agency, will affect about 2.5 million civil servants."Recreational applications do not deliver sufficient levels of cyber-security and data protection to be deployed on administration's digital tools," Civil Service Minister Stanislas Guerini said."These applications may therefore constitute a risk to the data protection of these administrations and their public officials." Although, exceptions could be granted for "institutional communications" purposes.So it looks like France, unlike the rest of the West, is viewing Chinese and American technology companies in a similar way. Although, it is not the first instance of technology tension between the US and France.In 2019, France approved a digital-services tax despite threats of retaliation by the US, who argued it unfairly targeted American technology giants such as Google and Facebook.And since 2018, France has been vocally opposed to the US Cloud Act, which gives American law-enforcement authorities the power to request data stored by most major cloud providers, even if it is outside the US.France has had a prominent voice in the Gaia-X project - set up to:address the conflict between the US Cloud Act and the EU General Data Protection Regulation (GDPR)set European cloud standards ensure customers' data is stored and processed in Europe, "immune" from non-European lawsDay after day, governments and public bodies are banning TikTok from staff devices - but France's decision to clamp down on all "recreational apps" stands out and is being praised by privacy campaigners.Fears about TikTok have reached fever pitch - but alongside the furore, people are beginning to question the privacy practices of other apps too - regardless of where the companies are based. And this feels like a moment privacy campaigners have been waiting for.Questions are being asked about how data is collected and used by the likes of Facebook, Instagram, Snapchat and even Candy Crush.The political focus is currently on TikTok because its parent company is in China - but the French government is clearly saying all these social-media companies have questions to answer.Related TopicsFrench politicsTikTokFranceChinaAppsMore on this storyWhat could a TikTok ban mean for creators?23 MarchFive takeaways from TikTok CEO's Congress grilling7 days agoHow a TikTok ban would - or wouldn't - work in practice23 March

Google faces new multi-billion advertising lawsuitPublished2 hours agoShareclose panelShare pageCopy linkAbout sharingImage source, AFPBy Chris VallanceTechnology reporterA lawsuit has been filed against Google to seek £3.4bn ($4.2bn) in compensation for publishers for lost revenue.The claim, by ex-Guardian technology editor Charles Arthur, alleges Google unlawfully used a dominant position in online adverts in a way that reduced what publishers could make from them.Google said it would fight the "speculative and opportunistic" action vigorously.It is the second such lawsuit, after a similar case was launched in November.That was brought by former Ofcom director Claudio Pollack, who is looking for up to £13.6bn in damages from the tech giant.The cases concern advertising technology - adtech - that decides in a fraction of a second which online adverts consumers will see, how much they will cost, and how much publishers will earn.Online display advertising is the main source of income for many websites.Google faces €25bn legal action in UK and the EUThe UK competition regulator, the Competition and Markets Authority (CMA), is also investigating Google's dominance in advertising technology. In the lawsuit, which was filed on Thursday, Mr Arthur claims that because of Google's abuse of its position, the prices of adtech services were inflated, and ad sales revenues of publishers were unlawfully reduced. "The CMA is currently investigating Google's anti-competitive conduct in adtech, but they don't have the power to make Google compensate those who have lost out. We can only right that wrong through the courts, which is why I am bringing this claim," he wrote.Collective claimsBoth legal claims ask the court - the Competition Appeal Tribunal -  to certify their claims as "opt-out", meaning every relevant publisher would be automatically included in the case unless they choose otherwise.These are collective claims, often referred to as a class action in the United States, which only became possible in the UK in 2015. Because they are brought on behalf of a whole group or class, the damages can be very large.Unless Mr Arthur and Mr Pollack agree to collaborate, the tribunal will have to decide which one should lead the collective claimMany competitorsGoogle told the BBC its advertising tools, "and those of our many adtech competitors, help millions of websites and apps fund their content, and enable businesses of all sizes to effectively reach new customers".Although the CMA found that Google owned the largest provider in three key areas of adtech, the firm maintains it has many competitors. It also says its adtech fees are lower than, or match, industry averages.But in a case launched in January, the US Justice department accused Google of being an "industry behemoth" that had "corrupted legitimate competition in the adtech industry by engaging in a systematic campaign to seize control of the wide swath of high-tech tools used by publishers".On Tuesday, Google asked a court to dismiss the case - arguing that the US government had overstated its hold on the market. In 2021 the French competition regulator, Autorité de la concurrence, fined Google €220m for favouring its own services in the online advertising sector. Related TopicsCompetition and Markets AuthorityGoogleAdvertisingMore on this storyUS accuses Google of 'driving out' ad rivals24 JanuaryGoogle probed by competition watchdog26 May 2022

Why you may have a thinking digital twin within a decadePublished13 June 2022Shareclose panelShare pageCopy linkAbout sharingImage source, Getty ImagesImage caption, Some experts say that thinking digital twins of humans may be just a decade awayBy Jane WakefieldTechnology reporterMost of us have been told by a friend that we have a doppelganger - some stranger they passed on the street who bore an uncanny resemblance to you.But imagine if you could create your very own twin, an exact copy of yourself, but one that lived a purely digital life?We are living in an age where everything that exists in the real world is being replicated digitally - our cities, our cars, our homes, and even ourselves.And just like the hugely-hyped metaverse - plans for a virtual, digital world where an avatar of yourself would walk around  - digital twins have become a new, talked-about tech trend.A digital twin is an exact replica of something in the physical world, but with a unique mission - to help improve, or in some other way provide feedback to, the real-life version.Initially such twins were just sophisticated 3D computer models, but artificial intelligence (AI) combined with the internet of things - which uses sensors to connect physical things to the network - have meant that you can now build something digitally that is constantly learning from and helping improve the real counterpart.Technology analyst Rob Enderle believes that we will have the first versions of thinking human digital twins "before the end of the decade".Image source, Intel Free PressImage caption, Rob Enderle says that there are ethical considerations to study as digital human twins are developed"The emergence of these will need a huge amount of thought and ethical consideration, because a thinking replica of ourselves could be incredibly useful to employers," he says."What happens if your company creates a digital twin of you, and says 'hey, you've got this digital twin who we pay no salary to, so why are we still employing you?'?Mr Enderle thinks that ownership of such digital twins will become one of the defining questions of the impending metaverse era.We have already started the journey towards human twinning - in the form of the above mentioned avatars - but these are currently rather clunky and primitive. In Meta's (formerly Facebook) virtual reality platform, Horizon Worlds, for example, you may be able to give your avatar a similar face to your own, but you can't even provide it with any legs because the technology is at such early stages.Prof Sandra Wachter, a senior research fellow in AI at Oxford University, understands the appeal of creating digital twins of humans, "it is reminiscent of exciting science fiction novels, and at the moment that is the stage where it is at".Image source, Sandra WachterImage caption, Prof Wachter says that digital human twins remain just science fiction at this stageShe adds that whether someone will "be successful at law school, get sick, or commit a crime - will depend on the still debated 'nature versus nurture question'. It will depend on good luck and bad luck, friends, family, their socio-economic background and environment, and of course their personal choices."   However, she explains, AI is not yet good at predicting these "single social events, due to their inherent complexity. And so, we have a long ways to go until we can understand and model a person's life from beginning to end, assuming that is ever possible."Instead, it is in the fields of product design, distribution and urban planning where the use of digital twins is currently the most sophisticated and extensive.In Formula One racing, the McLaren and Red Bull teams use digital twins of their race cars. Meanwhile, delivery giant, DHL, is creating a digital map of its warehouse and supply chains to allow it to be more efficient.Image source, Getty ImagesImage caption, McLaren has a digital twin of its latest car, which was used to aid its developmentAnd increasingly our cities are being replicated in the digital world; Shanghai and Singapore both have digital twins, set up to help improve the design and operations of buildings, transport systems and streets.In Singapore, one of the tasks of its digital twin is to help find new ways for people to navigate, avoiding areas of pollution. Other places use the technology to suggest where to build new infrastructure such as underground lines. And new cities in the Middle East are being built simultaneously in the real world and the digital.French software company, Dassault Systemes, says it is now seeing interest from thousands of firms for its digital twins technology.So far its work has included using digital twins to help a hair care firm digitally design more sustainable shampoo bottles, instead of endless real-life prototyping. This cuts down on waste. Image source, Getty ImagesImage caption, Authorities in Shanghai, pictured, have a digital twin of the city which they use to model its future developmentAnd it is enabling other firms to design new futuristic projects - from jetpacks, to motorbikes that have floating wheels, and even flying cars. Each has a physical prototype too, but the refining of that initial model happens in the digital space.But the real value seen in digital twins is in healthcare. Dassault Systemes' Living Heart project has created an accurate virtual model of the a human heart that can be tested and analysed, allowing surgeons to play out a series of "what if" scenarios for the organ, using various procedures and medical devices.The project was founded by Dr Steve Levine, who had personal reasons to want to create a digital twin. His daughter was born with congenital heart disease, and a few year's back, when she was in her late 20s and at high risk of heart failure, he decided to recreate her heart in virtual reality.Boston Children's Hospital is now using this technology to map out real patient heart conditions, while at Great Ormond Street hospital in London, a team of engineers is working with clinicians to test devices that may help children with rare and difficult-to-treat heart conditions.New Tech Economy is a series exploring how technological innovation is set to shape the new emerging economic landscape.Experimenting on a digital heart also has the knock-on effect of cutting down on the need to test on animals - one of the more controversial aspects of scientific research, says Severine Trouillet, global affairs director at Dassault Systemes.The firm now plans more digital organ twins, including the eye and even the brain."At some point we will all have a digital twin, so that you can go to the doctor, and we can increasingly make preventative medicine, and make sure that every treatment is personalised," says Ms Trouillet.Perhaps even more ambitious than replicating human organs is the race to build a digital version of our entire planet.US software firm, Nvidia, runs a platform called Omniverse, designed to create virtual worlds and digital twins.One of its most ambitious projects is to build a digital doppelganger of the Earth, capturing high resolution imagery of its entire surface.Image source, NvidiaImage caption, A graphic illustration of Earth-2, which aims to help track and tackle climate changeEarth-2, as it is dubbed, will use a combination of deep-learning models and neural networks to mimic physical environments in the digital sphere, and come up with solutions to climate change.In March this year, the European Commission, in conjunction with the European Space Agency among others, announced its own plans to make a digital twin of the planet, dubbed Destination Earth.By the end of 2024, it hopes to have enough data from real-time observations and simulations to have a digital twin that will focus on floods, drought and heatwaves, alongside natural disasters such as earthquakes, volcanic eruptions and tsunamis, and provide countries with concrete plans to save lives in the face of these growing challenges.Related TopicsUrban planningArtificial intelligence

UK rules out new AI regulatorPublished2 days agoShareclose panelShare pageCopy linkAbout sharingImage source, Getty ImagesBy Shiona McCallumTechnology reporterThe government has set out plans to regulate artificial intelligence with new guidelines on "responsible use".Describing it as one of the "technologies of tomorrow", the government said AI contributed £3.7bn ($5.6bn) to the UK economy last year.Critics fear the rapid growth of AI could threaten jobs or be used for malicious purposes.The term AI covers computer systems able to do tasks that would normally need human intelligence.This includes chatbots able to understand questions and respond with human-like answers, and systems capable of recognising objects in pictures.A new white paper from the Department for Science, Innovation and Technology proposes rules for general purpose AI, which are systems that can be used for different purposes.Technologies include, for example, those which underpin chatbot ChatGPT.As AI continues developing rapidly, questions have been raised about the future risks it could pose to people's privacy, their human rights or their safety. There is concern that AI can display biases against particular groups if trained on large datasets scraped from the internet which can include racist, sexist and other undesirable material. AI could also be used to create and spread misinformation.As a result many experts say AI needs regulation.However AI advocates say the tech is already delivering real social and economic benefits for people.And the government fears organisations may be held back from using AI to its full potential because a patchwork of legal regimes could cause confusion for businesses trying to comply with rules. Instead of giving responsibility for AI governance to a new single regulator, the government wants existing regulators - such as the Health and Safety Executive, Equality and Human Rights Commission and Competition and Markets Authority - to come up with their own approaches that suit the way AI is actually being used in their sectors.These regulators will be using existing laws rather than being given new powers.Michael Birtwistle, associate director from the Ada Lovelace Institute, carries out independent research, and said he welcomed the idea of regulation but warned about "significant gaps" in the UK's approach which could leave harms unaddressed. "Initially, the proposals in the white paper will lack any statutory footing. This means no new legal obligations on regulators, developers or users of AI systems, with the prospect of only a minimal duty on regulators in future. "The UK will also struggle to effectively regulate different uses of AI across sectors without substantial investment in its existing regulators," he said.The white paper outlines five principles that the regulators should consider to enable the safe and innovative use of AI in the industries they monitor:  • Safety, security and robustness: applications of AI should function in a secure, safe and robust way where risks are carefully managed• Transparency and "explainability": organisations developing and deploying AI should be able to communicate when and how it is used and explain a system's decision-making process in an appropriate level of detail that matches the risks posed by the use of AI• Fairness: AI should be used in a way which complies with the UK's existing laws, for example on equalities or data protection, and must not discriminate against individuals or create unfair commercial outcomes• Accountability and governance: measures are needed to ensure there is appropriate oversight of the way AI is being used and clear accountability for the outcomes • Contestability and redress: people need to have clear routes to dispute harmful outcomes or decisions generated by AIOver the next year, regulators will issue practical guidance to organisations to set out how to implement these principles in their sectors. Science, innovation and technology secretary Michelle Donelan said: "Artificial intelligence is no longer the stuff of science fiction, and the pace of AI development is staggering, so we need to have rules to make sure it is developed safely."But Simon Elliott, partner at law firm Dentons told the BBC the government's approach was a "light-touch" that makes the UK "an outlier" against the global trends around AI regulation.China, for example, has taken the lead in moving AI regulations past the proposal stage with rules that mandate companies notify users when an AI algorithm is playing a role."Numerous countries globally are developing or passing specific laws to address perceived AI risks - including algorithmic rules passed in China or the USA," continued Mr Elliott. He warned about the concerns that consumer groups and privacy activists will have over the risks to society "without detailed, unified regulation."He is also worried that the UK's regulators could be burdened with "an increasingly large and diverse" range of complaints, when "rapidly developing and challenging" AI is added to their workloads. In the EU, the European Commission has published proposals for regulations titled the Artificial Intelligence Act which would have a much broader scope than China's enacted regulation.They include "grading" AI products according to how potentially harmful they might be and staggering regulation accordingly. So for example an email spam filter would be more lightly regulated than something designed to diagnose a medical conditions - and some AI uses, such as social grading by governments, would be prohibited altogether."AI has been around for decades but has reached new capacities fuelled by computing power," Thierry Breton, the EU's Commissioner for Internal Market, said in a statement. The AI Act aims to "strengthen Europe's position as a global hub of excellence in AI from the lab to the market, ensure that AI in Europe respects our values and rules, and harness the potential of AI for industrial use," Mr Breton added.Meanwhile in the US The Algorithmic Accountability Act 2022 requires companies to assess the impacts of AI but the nation's AI framework is so far voluntary.Related TopicsArtificial intelligenceMore on this storyAI could affect 300 million jobs - report2 days agoNew biometrics laws urgently needed, review finds29 June 2022ChatGPT-style tech brought to Microsoft 36516 MarchNew chatbot has everyone talking to it7 December 2022

