{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved sports to CSV\n",
      "Successfully saved sports to CSV\n",
      "Successfully saved sports to CSV\n",
      "Successfully saved business to CSV\n",
      "Successfully saved business to CSV\n",
      "Successfully saved business to CSV\n",
      "Successfully saved technology to CSV\n",
      "Successfully saved technology to CSV\n",
      "Successfully saved technology to CSV\n",
      "Successfully saved technology to CSV\n",
      "Successfully saved sports to CSV\n",
      "Successfully saved sports to CSV\n",
      "Successfully saved politics to CSV\n",
      "Successfully saved politics to CSV\n",
      "Successfully saved politics to CSV\n",
      "Successfully saved politics to CSV\n",
      "Successfully saved health to CSV\n",
      "Successfully saved health to CSV\n",
      "Successfully saved health to CSV\n",
      "Successfully saved health to CSV\n",
      "Successfully saved climate to CSV\n",
      "Successfully saved climate to CSV\n",
      "Successfully saved climate to CSV\n",
      "Successfully saved sports to CSV\n",
      "Successfully saved climate to CSV\n",
      "Successfully saved climate to CSV\n",
      "Successfully saved climate to CSV\n",
      "Successfully saved climate to CSV\n",
      "Successfully saved business to CSV\n",
      "Successfully saved business to CSV\n",
      "Successfully saved business to CSV\n",
      "Successfully saved sports to CSV\n",
      "Successfully saved politics to CSV\n",
      "Successfully saved technology to CSV\n",
      "Successfully saved technology to CSV\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "\n",
    "# Load URLs from JSON file\n",
    "with open('urls.json', 'r') as f:\n",
    "    urls = json.load(f)\n",
    "\n",
    "# open a CSV file for writing\n",
    "with open('news_articles.csv', mode='w', newline='') as csvfile:\n",
    "    # define the fieldnames for the CSV writer\n",
    "    fieldnames = ['Topic', 'Content']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()  # write the header row to the CSV file\n",
    "\n",
    "    for url in urls:\n",
    "        url_string = url[\"url\"]\n",
    "        r = requests.get(url_string)\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            article = soup.find(\"article\")\n",
    "            # Get the text out of the soup and print it\n",
    "            content = article.get_text()\n",
    "            # print(content)\n",
    "\n",
    "            # write the topic and content to the CSV file\n",
    "            writer.writerow({'Topic': url['title'], 'Content': content})\n",
    "            print(f'Successfully saved {url[\"title\"]} to CSV')\n",
    "        else:\n",
    "            print(f\"Error: {r.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/silver/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved preprocessed data to csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load the news corpus into a Pandas DataFrame\n",
    "df = pd.read_csv('news_articles.csv')\n",
    "\n",
    "# Initialize NLTK components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_text(Content):\n",
    "    # Remove punctuation\n",
    "    Content = Content.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove digits\n",
    "    Content = re.sub(r'\\d+', '', Content)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(Content.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    # Lemmatize the remaining words\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "    # Rejoin the words into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the preprocessing function to the news corpus and save into a new CSV file\n",
    "df['Content'] = df['Content'].apply(preprocess_text)\n",
    "df.to_csv('preprocessed_news_corpus.csv', index=False)\n",
    "print(f'Successfully saved preprocessed data to csv')\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized using TF-IDF Vectorizer and adjusting the parameters to improve the quality of the features.\n",
    "Adjusted min_df=4 meaning any word that appears in the corpus less than 4 times will be excluded from the vocabulary.\n",
    "\n",
    "Training and testing using Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "Precision: 0.619047619047619\n",
      "Recall: 0.7142857142857143\n",
      "F1-Score: 0.6428571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silver/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NB_classifier.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Load the preprocessed news corpus\n",
    "df = pd.read_csv('preprocessed_news_corpus.csv')\n",
    "\n",
    "# Create a TF-IDF Vectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# vectorizer = TfidfVectorizer(max_df=0.5)\n",
    "vectorizer = TfidfVectorizer(min_df=4)\n",
    "\n",
    "\n",
    "# Vectorize the text data\n",
    "X = vectorizer.fit_transform(df['Content'])\n",
    "\n",
    "# Get the labels\n",
    "y = df['Topic']\n",
    "\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the categories for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'Tfidf_vectorizer.pkl')\n",
    "\n",
    "# Save the classifier\n",
    "joblib.dump(clf, 'NB_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climate\n",
      "sports\n",
      "technology\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the vectorizer and the classifier\n",
    "vectorizer = joblib.load('Tfidf_vectorizer.pkl')\n",
    "clf = joblib.load('NB_classifier.pkl')\n",
    "\n",
    "# Predict with new unseen data\n",
    "new_data = [\n",
    "    'There are multiple, feasible and effective options to reduce greenhouse gas emissions and adapt to human-caused climate change, and they are available now, said scientists in the latest report released by the Intergovernmental Panel on Climate Change (IPCC) on 20 March. â€œThis Synthesis Report underscores the urgency of taking more ambitious action and shows that, if we act now, we can still secure a liveable sustainable future for all,',\n",
    "    'There are 20 clubs in the Premier League. During the course of a season (from August to May) each club plays the others twice (a double round-robin system), once at their home stadium and once at that of their opponents, for 38 games. Teams receive three points for a win and one point for a draw. No points are awarded for a loss. Teams are ranked by total points, then goal difference, and then goals scored. If still equal, teams are deemed to occupy the same position. If there is a tie for the championship, for relegation, or for qualification to other competitions,',\n",
    "    'Innovations here include AI engineering, decision intelligence, operational AI systems, ModelOps, AI cloud services, smart robots, natural language processing (NLP), autonomous vehicles, intelligent applications and computer vision.'\n",
    "    ]\n",
    "\n",
    "new_data_transformed = vectorizer.transform(new_data)\n",
    "predicted_labels = clf.predict(new_data_transformed)\n",
    "for label in predicted_labels:\n",
    "    print(label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
